{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inteligência Artificial\n",
    "## Aluno: Flávio Correia de Sousa Filho\n",
    "## Matrícula: 397739\n",
    "\n",
    "## Trabalho de classificação e análise\n",
    "### O conjunto de dados escolhidos é um que remete a classificação de gestos musculares  do usuário para saber a ação de uma prótese. Ele vem com 4 arquivos .csv, cada arquivo tem 65 colunas, a ultima remetente a classe e as outras relacionadas a captura de um sensor.\n",
    "### As quatro classes representadas são os gestos manuais pedra, papel, tesoura e ok.\n",
    "\n",
    "### As colunas dos datasets foram nomeadas manualmente como mrXsY onde X é a leitura muscular e Y é o sensor\n",
    "\n",
    "### Os modelos utilizados foram:\n",
    "### 1- KNN\n",
    "### 2- Logistic Regression\n",
    "### 3- Random Forest\n",
    "### 4- Adaboost\n",
    "### 5- Gaussian Naive Bayes\n",
    "\n",
    "### As métricas utilizadas foram a matriz de confusão, precision, recall, f1-score, conjunto de médias e acurácia\n",
    "\n",
    "\n",
    "\n",
    "### Link do dataset com mais informações: https://www.kaggle.com/kyr7plus/emg-4#0.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemos os quatro datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = pd.read_csv(\"0.csv\")\n",
    "data1 = pd.read_csv(\"1.csv\")\n",
    "data2 = pd.read_csv(\"2.csv\")\n",
    "data3 = pd.read_csv(\"3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2910, 65), (2903, 65), (2943, 65), (2922, 65))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0.shape, data1.shape, data2.shape, data3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vemos que eles não estão balanceados e isso pode prejudicar o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = data0.iloc[:2904, :]\n",
    "data2 = data2.iloc[:2904, :]\n",
    "data3 = data3.iloc[:2904, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2904, 65), (2903, 65), (2904, 65), (2904, 65))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0.shape, data1.shape, data2.shape, data3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora com bases de mesmo tamanho, vamos separar os dados X0, X1, X2 e X3 das classes y0, y1, y2 e y3. \n",
    "### Depois juntamos todos os Xs e ys e realizamos a divisão em treino e teste\n",
    "### Para o treino 80% e o  para o teste 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = data0.iloc[:, :-1]\n",
    "y0 = data0.iloc[:, -1]\n",
    "X1 = data1.iloc[:, :-1]\n",
    "y1 = data1.iloc[:, -1]\n",
    "X2 = data2.iloc[:, :-1]\n",
    "y2 = data2.iloc[:, -1]\n",
    "X3 = data3.iloc[:, :-1]\n",
    "y3 = data3.iloc[:, -1]\n",
    "\n",
    "X = pd.concat([X0, X1, X2, X3])\n",
    "y = pd.concat([y0, y1, y2, y3])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizando os dados para realizar a comparação com dados não normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = std_scale.transform(X_train)\n",
    "X_test_std  = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para o KNN geramos uma lista com possíveis K valores e escolheremos aquele com melhor acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "kList = [int(i) for i in range(3,20,2)]\n",
    "valores = [[], []]\n",
    "for kNumber in kList:\n",
    "    clfKNN = neighbors.KNeighborsClassifier(kNumber, weights = 'uniform')\n",
    "    trained_model = clfKNN.fit(X_train, y_train)\n",
    "    valores[0] += [kNumber]\n",
    "    valores[1] += [trained_model.score(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O melhor valor para K foi 7 obtendo acurácia de 0.6840292724924666\n"
     ]
    }
   ],
   "source": [
    "bestK = 0\n",
    "for i in range(1, len(valores[0])):\n",
    "    if valores[1][bestK] < valores[1][i]:\n",
    "        bestK = i\n",
    "\n",
    "print(\"O melhor valor para K foi\",valores[0][bestK],\"obtendo acurácia de\",valores[1][bestK])\n",
    "\n",
    "clfKNN = neighbors.KNeighborsClassifier(valores[0][bestK], weights = 'uniform')\n",
    "trained_model = clfKNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predito = trained_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSÃO\n",
      "[[527  49   8  12]\n",
      " [  0 538   3  44]\n",
      " [  0 243  99 257]\n",
      " [  3 112   3 425]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.94       596\n",
      "           1       0.57      0.92      0.70       585\n",
      "           2       0.88      0.17      0.28       599\n",
      "           3       0.58      0.78      0.66       543\n",
      "\n",
      "    accuracy                           0.68      2323\n",
      "   macro avg       0.75      0.69      0.65      2323\n",
      "weighted avg       0.76      0.68      0.64      2323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predito)\n",
    "print(\"MATRIZ DE CONFUSÃO\")\n",
    "print(cm)\n",
    "\n",
    "r = classification_report(y_test, predito)\n",
    "print(\"\\n\\n\"+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados normalizados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = [[], []]\n",
    "for kNumber in kList:\n",
    "    clfKNN = neighbors.KNeighborsClassifier(kNumber, weights = 'uniform')\n",
    "    trained_model = clfKNN.fit(X_train_std, y_train)\n",
    "    valores[0] += [kNumber]\n",
    "    valores[1] += [trained_model.score(X_test_std, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O melhor valor para K foi 7 obtendo acurácia de 0.6577701248385708\n"
     ]
    }
   ],
   "source": [
    "bestK = 0\n",
    "for i in range(1, len(valores[0])):\n",
    "    if valores[1][bestK] < valores[1][i]:\n",
    "        bestK = i\n",
    "\n",
    "print(\"O melhor valor para K foi\",valores[0][bestK],\"obtendo acurácia de\",valores[1][bestK])\n",
    "clfKNN = neighbors.KNeighborsClassifier(valores[0][bestK], weights = 'uniform')\n",
    "trained_model = clfKNN.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSÃO\n",
      "[[371 120  37  68]\n",
      " [  0 550   0  35]\n",
      " [  3 200 190 206]\n",
      " [ 14 111   1 417]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.62      0.75       596\n",
      "           1       0.56      0.94      0.70       585\n",
      "           2       0.83      0.32      0.46       599\n",
      "           3       0.57      0.77      0.66       543\n",
      "\n",
      "    accuracy                           0.66      2323\n",
      "   macro avg       0.73      0.66      0.64      2323\n",
      "weighted avg       0.74      0.66      0.64      2323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predito = trained_model.predict(X_test_std)\n",
    "cm = confusion_matrix(y_test, predito)\n",
    "print(\"MATRIZ DE CONFUSÃO\")\n",
    "print(cm)\n",
    "\n",
    "r = classification_report(y_test, predito)\n",
    "print(\"\\n\\n\"+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vemos que para o KNN a normalização melhora a matriz de confusão em geral e na detecção da classe 0, mas piora a acurácia e algumas outras métricas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\flavi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\flavi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clfLR = LogisticRegression()\n",
    "modelLR = clfLR.fit(X_train, y_train)\n",
    "predito = modelLR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSÃO\n",
      "[[268 104 106 118]\n",
      " [ 47 194 170 174]\n",
      " [ 97 164 158 180]\n",
      " [ 94 124 126 199]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.45      0.49       596\n",
      "           1       0.33      0.33      0.33       585\n",
      "           2       0.28      0.26      0.27       599\n",
      "           3       0.30      0.37      0.33       543\n",
      "\n",
      "    accuracy                           0.35      2323\n",
      "   macro avg       0.36      0.35      0.35      2323\n",
      "weighted avg       0.36      0.35      0.36      2323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predito)\n",
    "print(\"MATRIZ DE CONFUSÃO\")\n",
    "print(cm)\n",
    "\n",
    "r = classification_report(y_test, predito)\n",
    "print(\"\\n\\n\"+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados normalizados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\flavi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\flavi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clfLR = LogisticRegression()\n",
    "modelLR = clfLR.fit(X_train_std, y_train)\n",
    "predito = modelLR.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSÃO\n",
      "[[268 104 106 118]\n",
      " [ 47 194 169 175]\n",
      " [ 97 163 159 180]\n",
      " [ 94 124 126 199]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.45      0.49       596\n",
      "           1       0.33      0.33      0.33       585\n",
      "           2       0.28      0.27      0.27       599\n",
      "           3       0.30      0.37      0.33       543\n",
      "\n",
      "    accuracy                           0.35      2323\n",
      "   macro avg       0.36      0.35      0.35      2323\n",
      "weighted avg       0.36      0.35      0.36      2323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predito)\n",
    "print(\"MATRIZ DE CONFUSÃO\")\n",
    "print(cm)\n",
    "\n",
    "r = classification_report(y_test, predito)\n",
    "print(\"\\n\\n\"+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Não houve diferença entre dados normalizados e não, ambos apresentaram resultados não satisfatórios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando entropy como critério"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\flavi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(criterion='entropy', random_state=0)\n",
    "model = clf.fit(X_train, y_train)\n",
    "predito = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSÃO\n",
      "[[578   0  13   5]\n",
      " [  0 526  20  39]\n",
      " [ 16  15 532  36]\n",
      " [ 25  32  35 451]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       596\n",
      "           1       0.92      0.90      0.91       585\n",
      "           2       0.89      0.89      0.89       599\n",
      "           3       0.85      0.83      0.84       543\n",
      "\n",
      "    accuracy                           0.90      2323\n",
      "   macro avg       0.90      0.90      0.90      2323\n",
      "weighted avg       0.90      0.90      0.90      2323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predito)\n",
    "print(\"MATRIZ DE CONFUSÃO\")\n",
    "print(cm)\n",
    "\n",
    "r = classification_report(y_test, predito)\n",
    "print(\"\\n\\n\"+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mudando o critério para gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\flavi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(criterion='gini', random_state=0)\n",
    "model = clf.fit(X_train, y_train)\n",
    "predito = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSÃO\n",
      "[[574   0  13   9]\n",
      " [  0 524  19  42]\n",
      " [ 15  23 536  25]\n",
      " [ 33  33  30 447]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       596\n",
      "           1       0.90      0.90      0.90       585\n",
      "           2       0.90      0.89      0.90       599\n",
      "           3       0.85      0.82      0.84       543\n",
      "\n",
      "    accuracy                           0.90      2323\n",
      "   macro avg       0.89      0.89      0.89      2323\n",
      "weighted avg       0.90      0.90      0.90      2323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predito)\n",
    "print(\"MATRIZ DE CONFUSÃO\")\n",
    "print(cm)\n",
    "\n",
    "r = classification_report(y_test, predito)\n",
    "print(\"\\n\\n\"+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vemos que o random forest tem um desempenho muito bom em relação aos outros modelos, e que tendo gini como critério obtemos melhores resultados na maioria das métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criamos duas listas com possíveis parâmetros do modelo e escolheremos o melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrList = [0.5 , 1 , 2]\n",
    "nestList = [50, 100, 150]\n",
    "valores = [[], []]\n",
    "for lrEl in lrList:\n",
    "    for nestEl in nestList:\n",
    "        clf = AdaBoostClassifier(n_estimators=nestEl, learning_rate=lrEl, random_state=0)\n",
    "        model = clf.fit(X_train, y_train)\n",
    "        valores[0] += [(lrEl, nestEl)]\n",
    "        valores[1] += [model.score(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.5, 50),\n",
       "  (0.5, 100),\n",
       "  (0.5, 150),\n",
       "  (1, 50),\n",
       "  (1, 100),\n",
       "  (1, 150),\n",
       "  (2, 50),\n",
       "  (2, 100),\n",
       "  (2, 150)],\n",
       " [0.8024106758501938,\n",
       "  0.8519156263452432,\n",
       "  0.8738699956952217,\n",
       "  0.8161859664227292,\n",
       "  0.8527765820060267,\n",
       "  0.8885062419285407,\n",
       "  0.5458458889367198,\n",
       "  0.5458458889367198,\n",
       "  0.5458458889367198]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O melhor valor para o estimador foi 50 e para o learning rate foi 0.5\n"
     ]
    }
   ],
   "source": [
    "bestP = 0\n",
    "for i in range(1, len(valores[1])):\n",
    "    if valores[1][bestK] < valores[1][i]:\n",
    "        bestK = i\n",
    "\n",
    "print(\"O melhor valor para o estimador foi\",valores[0][bestP][1],\"e para o learning rate foi\",valores[0][bestP][0])\n",
    "clf = AdaBoostClassifier(n_estimators=valores[0][bestP][1], learning_rate=valores[0][bestP][0], random_state=0)\n",
    "model = clf.fit(X_train, y_train)\n",
    "predito = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSÃO\n",
      "[[383   0  77 136]\n",
      " [  0 537  13  35]\n",
      " [  0  30 531  38]\n",
      " [ 11  98  21 413]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.64      0.77       596\n",
      "           1       0.81      0.92      0.86       585\n",
      "           2       0.83      0.89      0.86       599\n",
      "           3       0.66      0.76      0.71       543\n",
      "\n",
      "    accuracy                           0.80      2323\n",
      "   macro avg       0.82      0.80      0.80      2323\n",
      "weighted avg       0.82      0.80      0.80      2323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predito)\n",
    "print(\"MATRIZ DE CONFUSÃO\")\n",
    "print(cm)\n",
    "\n",
    "r = classification_report(y_test, predito)\n",
    "print(\"\\n\\n\"+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vemos que o Adabost tem um desempenho satisfatório em relação às métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "model = clf.fit(X_train, y_train)\n",
    "predito = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSÃO\n",
      "[[549   0   4  43]\n",
      " [  0 556  14  15]\n",
      " [  5  14 553  27]\n",
      " [ 39  81   8 415]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92       596\n",
      "           1       0.85      0.95      0.90       585\n",
      "           2       0.96      0.92      0.94       599\n",
      "           3       0.83      0.76      0.80       543\n",
      "\n",
      "    accuracy                           0.89      2323\n",
      "   macro avg       0.89      0.89      0.89      2323\n",
      "weighted avg       0.89      0.89      0.89      2323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predito)\n",
    "print(\"MATRIZ DE CONFUSÃO\")\n",
    "print(cm)\n",
    "\n",
    "r = classification_report(y_test, predito)\n",
    "print(\"\\n\\n\"+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vemos que o GNB possui um rendimento satisfatório"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando esse problema e comparando os modelos utilizados, podemos afirmar que o melhor modelo foi o  Random Forest \n",
    "http://localhost:8888/notebooks/trabalhoClassificacao.ipynb#Random-Forest\n",
    "### E o pior foi de Regressão Logística.\n",
    "http://localhost:8888/notebooks/trabalhoClassificacao.ipynb#Regress%C3%A3o-Log%C3%ADstica"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
